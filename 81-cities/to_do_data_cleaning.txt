1. I’m debating whether to add country and UA to the massive year-agg metadata. But I probably should. I’ll just generate new files though, because I need to make ‘mezzo’ versions of the 2011-2013 data.

2. Parse timestamps, aggregate by day, month. Already have year. Probably can do some diurnal anaysis.

3. Ok this isn’t exactly data cleaning, but it’d be neat to plot all the Tweet points that AREN’T in the major cities.

4. Here’s a question: how many tweets are country-mislabeled by my UA polygons?

5. Also, I need to run the country join some other way. Fiona is taking too long. One thing I could try is to do multiple joins at the same time on chunks of the metadata. Fiona was designed for that.

6. Check on the date ranges. Is 2011 incomplete? How incomplete is 2014? This can be done with Mongo queries, I suppose, but eh.

7. 2011: 2 million. 2012: 30 million. 2013: 103.5 million. 2014: 133.8 million. Overall trend is an important comparison case. Note that we only have ~270 million actual tweets. The original database files have 300 million, but this must be what Mehrdad was able to get with his query.

8. Ok here’s how to do it: run some number of scripts at the same time, each working on a chunk. Save the new chunks to some new folder, and eliminate the columns I don’t need. Then combine them together into a single file, as before. Then break out into country files, save to the dirs I’ve already created. Then, I need to join on the UA facts to the large file. And then the UA are already broken out into separate files.

9. Just check total per year and then total in my UA data

